\documentclass[../main.tex]{subfiles}

\begin{document}
    \chapter{Experiments}\label{chap:experiments}

    \section{Implementation}\label{sec:implementation}
    We use Torch7 library~\cite{torch7} for implementation. The DMF building blocks (apart from standard layers) are the
    element-wise multiplication and the depth contact operation, which are already available in Torch7.
    Regarding Grad-CAM, we start from the code provided by the authors~\cite{gradcam-github} and we modify it as discussed
    in Section~\ref{subsec:gradcam-domainness}.
    We implement the Spatial Pyramid Pooling layer as an extension of Torch7, and we build the computational graph to compose
    the building blocks into the overall end-to-end architecture.
    \newline\newline
    For each experiment setup, we train and test three different network configurations. All three variants starts from a baseline network pretrained on
    the \textit{ImageNet} dataset~\cite{imagenet}.
    This procedure of starting with a network pretrained on a dataset and fine-tuning it on a new-but-similar dataset is called
    \textit{Transfer Learning}~\cite{transfer-learning} in the literature.
    In the first configuration (which we call Softmax), we replace only the last layer of the network with a new layer
    with dimension equal to the number of classes in the dataset. We train only this last layer, keeping all the other parameters frozen during training.
    In the second network (Spp), we remove all the fully-connected layers, as well as the last Max Pooling layer.
    We replace the latter with a Spatial Pyramid Pooling layer, and we add new fully-connected layers on top of it.
    The third network is DMF, whom architecture is discussed in Section~\ref{sec:dmf-network}. Note that in Spp and DMF the substitution of
    the fully-connected layers is necessary, as in a feed-forward network the removal of layer $i$ constrains us to remove also layers $i + 1$ to $n$.
    \\
    With this configuration of networks, we are able to verify the
    effectiveness of each component of the Domain-Multiplicative Fusion Network. In one experiment setting, we also compare our technique
    against state-of-the-art approaches.

    \section{Experiment settings}

    \subsection{Baseline Networks}
    In our experiments we use two baseline networks. The first is \textit{Alexnet}~\cite{alexnet}, winner of the ILSVRC 2012 competition
    and the responsible for the recent deep learning revolution in computer vision. The paper introduced many interesting innovations, most
    notably the use of Graphics Processing Units (GPUs) to train neural networks with millions of parameters at large scale, a method
    to reduce overfitting known as Dropout, and the use of the Rectified Linear Unit (ReLU) activation function.
    The second network is VGG16~\cite{vgg16}, winner of the ILSVRC 2014 competition.
    VGG16 is much deeper than Alexnet, in fact Alexnet has 5 convolutional layers and 3 fully-connected layers, while VGG16 has 13
    convolutional layers and 3 fully-connected layers. The use of two baseline networks improves the reliability of our findings,
    as this decreases the probability that our results were due to the use of a specific architecture.

    \subsection{Training procedure}
    Each network is trained using the same set of hyperparameters, established by using a validation set to search over a grid of
    possible values.
    The parameters of the first $f$ layers are frozen, where $f = 22$ for AlexNet (Softmax), $f = 38$ for VGG16 (Softmax),
    $f = 5$ for AlexNet (Spp and DMF), $f = 12$ for VGG16 (Spp and DMF).
    The parameters of layers $f + 1$ to $g$ are not frozen, but they are set with a small learning rate $\alpha_{f+1,g} = 10^{-5}$ (Spp and DMF)
    and $\alpha_{f+1,g} = 0$ (Softmax). $g$ is the number of layers that are retained from the ImageNet baseline.
    The parameters of layers $g + 1$ to $n$ have a higher learning rate $\alpha_{g+1,n} = 10^{-3}$ (Softmax) and $\alpha_{g+1,n} = 5 \times 10^{-4}$
    (Spp and DMF). Also, for all the layers that are trained from random initialization, a weight decay of $w = 5 \times 10^{-4}$ is used as a regularizer.
    In Spp and DMF, fully-connected layers have dimension $2048 \rightarrow 2048 \rightarrow 15 $ for AlexNet and $4096 \rightarrow 2048
    \rightarrow 15 $ for VGG16, and they uses dropout with $p = 0.6$.

    The optimizer used in the training procedure is Stochastic Gradient Descent with Momentum~\cite{momentum}. The momentum parameter
    is set to 0.9, with nesterov momentum~\cite{nesterov-momentum} enabled.
    We use a batch size of 256 for AlexNet and 16 for VGG16 (due to the high memory footprint of this network).
    Training was performed in parallel on multiple NVIDIA Titan X GPUs.

    \subsection{Datasets and Metrics}
    We evaluate our DMF on two variants of the iCub World dataset~\cite{icw}, which we call respectively iCW-translation and iCW-scale.
    Both datasets have two domains. As the names suggest, in the former case the shift between the two domains is caused by large
    object translations, while in the latter it is caused by scale variations. It is worth noting that, as discussed in Section~\ref{subsec:convnets},
    CNNs are known to handle some degree of scale and translation invariances. However, using a network structure specifically designed
    to handle such invariances have been shown to be more successful in many tasks~\cite{sppooling}. 
    In both datasets, one domain is composed by 4500 images and the other one by 3000 images. These are rather small
    datasets for data-hungry deep learning models, hence the choice of starting from ImageNet baselines.
    For iCW-translation, we also compare against prior state-of-the-art approaches.
    \newline\newline
    To measure performance, we train each network over one domain and use the other domain as the test set, measuring
    classification accuracy. In order to improve statistical robustness, we run all the
    experiments 5 times, and we report mean and standard deviation of the results.

    \subsection{The iCub World dataset}
    The iCW Transformations dataset~\cite{icw} contains images with objects from 15 categories, acquired through a robot camera.
    Each object is acquired while undergoing isolated visual transformations, in order to study invariance to real-world nuisances.
    A human operator moves the object holding it in the hand and the robot tracks it by exploiting either motion cues or depth cues.
    Different datasets are available depending on the object transformation carried out by the human.
    We will use two different variants of the iCW Transformations dataset:

    \begin{itemize}
        \item \textbf{Translation}: The human moves in a semi-circle around the iCub robot, keeping approximately the same
            distance and pose of the object in the hand with respect to the cameras. We use only images taken at the extreme of the
            semi-circle, thus the object appears either at the left of the image or at the right. Another cause of domain shift is
            that the background changes dramatically, while the object appearance remains the same.
        \item \textbf{Scaling}: The human moves the hand holding the object back and forth, thus changing the object's
            scale with respect to the cameras. In this case, the background remains almost the same, while the object
            appearance undergoes large changes due to the large scaling factor.
    \end{itemize}

    For each of the two variations the classes are perfectly balanced, namely each one has the same number of samples. Each dataset
    has two domains, hence it defines two domain adaptation tasks: the first is train on domain 1 and test on domain 2 and the second
    is the opposite.

    In figure~\ref{fig:icw-samples}, we can see a random sample of images from the dataset.

    \begin{figure}[h!]
        \centering{}
        \includegraphics[width=\linewidth]{./img/icw-samples.png}
        \caption{Random sample from the iCubWorld dataset.}\label{fig:icw-samples}
    \end{figure}

    \subsubsection{iCW Translation: Results}

    \begin{table}[h!]
        \centering{}
        \begin{tabular}{l c c r}
            \toprule
            Network  & \textit{Left 1} \rightarrow{} \textit{Left 2} & \textit{Left 2} \rightarrow{} \textit{Left 1} & Average \\
            \midrule
            Softmax  & $62.15 \pm{} 0.94$ & $64.74 \pm{} 0.92$ & 63.44 \\
            Spp      & $75.14 \pm{} 1.09$ & $75.91 \pm{} 1.26$ & 75.53 \\
            DANN     & $76.43$            & 54.60              & 65.52 \\
            DMF & $78.17 \pm{} 0.66$ & $77.30 \pm{} 0.80$ & 77.73 \\
            \bottomrule
        \end{tabular}
    \end{table}

    \subsubsection{iCW Scale: Results}

\end{document}
